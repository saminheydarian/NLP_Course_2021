{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Character-level Generation with Sequential Models"
      ],
      "metadata": {
        "id": "dIXxF8oFp_85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have two kinds of text generation:\n",
        "\n",
        "\n",
        "1.   Character-level\n",
        "2.   Word Level\n",
        "\n",
        "In this assignment we are focusing on the first one. Using a sequence of characters, we are going to train a model to predict the next character in the sequence. We will run you through the process step by step.\n",
        "\n",
        "First, you have to import some required packages by running the cell below.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M2koofrWqdGa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H4aXGPoUp7V5"
      },
      "outputs": [],
      "source": [
        "#@title Import Requireed Packages\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time for the our data. In this assignment we will be using the \"Shakespeare\" data."
      ],
      "metadata": {
        "id": "iR_3GoGkrn32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read and decode\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "metadata": {
        "id": "mygHErGws5N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the data (the first 250 characters)."
      ],
      "metadata": {
        "id": "Z_0a_Uv-s74b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "id": "p5VLrwVgtBCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to build up the vocabulary by finding the unique characters:"
      ],
      "metadata": {
        "id": "qJ5R4vfa4bVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "WRLgYlkB4a96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the text\n",
        "\n",
        "We now need to convert these strings to numerical representations so that our model can understand them. To do this you will need to use:\n",
        "\n",
        "```\n",
        "tf.keras.layers.StringLookup\n",
        "```\n",
        "and then pass it the vocabulary we created in the previous part.\n",
        "\n",
        "However, the text needs to be tokenized first.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qcwAxyW0s6sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ['keep', 'store']\n",
        "\n",
        "chars = tf.strings.unicode_split(sample_text, input_encoding='UTF-8')\n",
        "\n",
        "char2id = ''' Now use \"tf.keras.layers.StringLookup\" to convert text to ids '''\n",
        "\n",
        "ids = char2id(chars)\n",
        "\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "Kh3hyAHW5VIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our goal is to generate text and not just understand text, we need to convert machine understandable numeric data to human-understandable text. Therefore, we will need a method to convert these ids back to string.\n",
        "\n",
        "Use the same method you used for converting chars into id but this time use the option <font color='red'>invert=True</font>. "
      ],
      "metadata": {
        "id": "A01oJaW36ZIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2char = ''' Your code '''\n",
        "\n",
        "chars = id2char(ids)\n",
        "print(chars)\n",
        "\n",
        "# now we want to use a code to join chars into strings\n",
        "# Hint: You should use a method from tf.strings called reduce_join\n",
        "def id2text(ids):\n",
        "''' Your Code '''\n",
        "  pass"
      ],
      "metadata": {
        "id": "2MPsltu5DqJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the dataset"
      ],
      "metadata": {
        "id": "eq095wplFrok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we would like to divide our text into sequences. Each sequence will be constrained by <font color='red'>sequence_length</font> that we define.\n",
        "\n",
        "The corresponding target sequence of each input sequence has the same length except one character shifted to the right. So you should break the text into chunks of <font color='red'> seq_length+1</font> For instance, given a seq_length of 5 and \"Python\" as text, the input sequence would be \"pytho\" the target would be \"ython\".\n",
        "\n",
        "To do this you should use:\n",
        "```\n",
        "tf.data.Dataset.from_tensor_slices\n",
        "```\n",
        "to convert the text vector into a sequence of character indices.\n"
      ],
      "metadata": {
        "id": "v412nM1wFxYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = char2id(tf.strings.unicode_split(text, input_encoding='UTF-8'))\n",
        "print(all_ids)\n",
        "\n",
        "ids_dataset = ''' Your Code (use from_tensor_slices) '''\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(id2char(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "YrlxMdT_HkUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "metadata": {
        "id": "N9lToSqBVRd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use the <font color='red'>batch</font> method to convert these characters to sequences with the desired length."
      ],
      "metadata": {
        "id": "XulJjvGlVS13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ''' Your Code '''\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(id2char(seq))\n",
        "  \n",
        "# Changing tokens back to text\n",
        "for seq in sequences.take(5):\n",
        "  print(id2text(seq).numpy())"
      ],
      "metadata": {
        "id": "AG4-tH8IVnKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train your model you need aa dataset consisting pairs of (input, label), where input and label are sequences. Given each time step, an input is the current character and the label is the next character. Now write a function that takes a sequence input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "OtSr10QhV47C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = '''Your Code'''\n",
        "    target_text = '''Your Code'''\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "x1zQmHlrXLgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "example:"
      ],
      "metadata": {
        "id": "FUlKbDkJXWz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(split_input_target(list(\"Hello\")))\n",
        "\n",
        "# Should return:\n",
        "''' ([H, e, l, l],\n",
        "    [e, l, l, o]) '''"
      ],
      "metadata": {
        "id": "k70Jit5ZXYIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "v8s6rg7jYBGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", id2text(input_example).numpy())\n",
        "    print(\"Target:\", id2text(target_example).numpy())"
      ],
      "metadata": {
        "id": "PCJJu2nWYSzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating training batches\n",
        "\n",
        "Now you should shuffle the data and pack it into batches."
      ],
      "metadata": {
        "id": "ca7Lmz_ZZKuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size is used for shuffling the dataset\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = ('''Your code for shuffling and batching the data also use .prefetch(tf.data.experimental.AUTOTUNE)) at the end''')\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "f1X8Jcd_ZsQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model\n"
      ],
      "metadata": {
        "id": "MnDs0kdaabNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you should build your model. Please use the following for your model:\n",
        "\n",
        "\n",
        "*   An embedding layer\n",
        "*   An RNN layer (LSTM or GRU)\n",
        "*   Dense Layer\n",
        "\n"
      ],
      "metadata": {
        "id": "dslxli5ff1E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "xRmxCjA5fzxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GenModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = '''embedding layer'''\n",
        "    self.gru = '''GRU or LSTM layer'''\n",
        "    self.dense = '''dense'''\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    '''\n",
        "    pass the inputs through the embedding layer, the RNN layer, and then the dense\n",
        "    layer. You should also check for initial states \n",
        "    \n",
        "    '''\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "mQw5veIhgZjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(char2id.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "w-gWS1B1hXRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the model\n",
        "\n",
        "Now let's check our model to see if it behaves as expected."
      ],
      "metadata": {
        "id": "tRhnsajDhrH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "cOn-xz2Khyfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "8pkL7vAZh3tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "print(sample_indices)"
      ],
      "metadata": {
        "id": "EwzxiHCkh-md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", id2text(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", id2text(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "KHJyMJ3SiEnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "We have now built our model. The only part left is to train and then test the model.\n",
        "\n",
        "We need a loss function. Please choose the correct loss function from the followings:\n",
        "\n",
        "\n",
        "*   Categorical Cross Entropy\n",
        "*   Sparse Categorical Cross Entropy\n",
        "*   Binary Cross Entropy\n",
        "*   MSE"
      ],
      "metadata": {
        "id": "HTjxCVnwiL_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = ''' Your Code '''"
      ],
      "metadata": {
        "id": "l0h1o-1bjIS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "metadata": {
        "id": "fcBTEqIYjM2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean loss with no training:"
      ],
      "metadata": {
        "id": "GB8PxGcjjZK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "metadata": {
        "id": "rqf_G47XjeEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model:"
      ],
      "metadata": {
        "id": "sAVtH6tkjep3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "i5ypuFc7joKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure Checkpoints:"
      ],
      "metadata": {
        "id": "DaqY2LBYjhpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "Gx-ALXaZjmFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = ''' Train the model given the dataset, epochs, and callback '''"
      ],
      "metadata": {
        "id": "NkfhD3uPjyrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Texts"
      ],
      "metadata": {
        "id": "sOXZUCpGkFfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we write a class to generate characters based on the model we trained."
      ],
      "metadata": {
        "id": "2a57X8y-Kbw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneChar(tf.keras.Model):\n",
        "  def __init__(self, model, id2char, char2id, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = id2char\n",
        "    self.ids_from_chars = char2id\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape= '''Your Code'''\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_char(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    # Convert tokens to ids\n",
        "    input_ids = '''Your Code'''\n",
        "\n",
        "    # Run the model and get the states and predicted logits\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = '''Your Code'''\n",
        "\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = '''Your Code'''\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = '''Your Code'''\n",
        "\n",
        "    # Sample the output logits to generate token IDs (use random.categorical).\n",
        "    predicted_ids = '''Your Code'''\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = '''Your Code'''\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "n1u0o-jvmezu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_char_model = OneChar(model, id2char, char2id)"
      ],
      "metadata": {
        "id": "UwdouIS9oXz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "metadata": {
        "id": "QCxXBl0Moo0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = '''Your Code'''\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "7qxLixtYoohV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now play with the model and hyperparameters (epochs, ...) and run this again to see if the results have improved."
      ],
      "metadata": {
        "id": "EvmmtzvepHjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = '''Your Code'''\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "xsRiYKTLpU8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}