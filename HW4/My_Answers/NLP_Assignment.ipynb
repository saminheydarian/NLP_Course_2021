{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIXxF8oFp_85"
      },
      "source": [
        "# Character-level Generation with Sequential Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2koofrWqdGa"
      },
      "source": [
        "We have two kinds of text generation:\n",
        "\n",
        "\n",
        "1.   Character-level\n",
        "2.   Word Level\n",
        "\n",
        "In this assignment we are focusing on the first one. Using a sequence of characters, we are going to train a model to predict the next character in the sequence. We will run you through the process step by step.\n",
        "\n",
        "First, you have to import some required packages by running the cell below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4aXGPoUp7V5"
      },
      "outputs": [],
      "source": [
        "#@title Import Requireed Packages\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR_3GoGkrn32"
      },
      "source": [
        "Now it's time for the our data. In this assignment we will be using the \"Shakespeare\" data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mygHErGws5N4",
        "outputId": "2defdfe1-850c-4cd8-adc6-4049a2b59f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read and decode\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_0a_Uv-s74b"
      },
      "source": [
        "Let's take a look at the data (the first 250 characters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5VLrwVgtBCX",
        "outputId": "f3af33b0-800c-4be4-c9f9-1581f9498735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ5R4vfa4bVI"
      },
      "source": [
        "We now need to build up the vocabulary by finding the unique characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRLgYlkB4a96",
        "outputId": "b4a0910c-7a91-43c4-e974-962aafedeb99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcwAxyW0s6sS"
      },
      "source": [
        "## Preprocessing the text\n",
        "\n",
        "We now need to convert these strings to numerical representations so that our model can understand them. To do this you will need to use:\n",
        "\n",
        "```\n",
        "tf.keras.layers.StringLookup\n",
        "```\n",
        "and then pass it the vocabulary we created in the previous part.\n",
        "\n",
        "However, the text needs to be tokenized first.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh3hyAHW5VIh",
        "outputId": "5aa9c8e0-2c97-4cf5-b794-7e1a7198a420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[50, 44, 44, 55], [58, 59, 54, 57, 44]]>\n"
          ]
        }
      ],
      "source": [
        "sample_text = ['keep', 'store']\n",
        "\n",
        "chars = tf.strings.unicode_split(sample_text, input_encoding='UTF-8')\n",
        "\n",
        "char2id = tf.keras.layers.StringLookup(vocabulary=vocab)\n",
        "\n",
        "ids = char2id(chars)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A01oJaW36ZIH"
      },
      "source": [
        "Since our goal is to generate text and not just understand text, we need to convert machine understandable numeric data to human-understandable text. Therefore, we will need a method to convert these ids back to string.\n",
        "\n",
        "Use the same method you used for converting chars into id but this time use the option <font color='red'>invert=True</font>. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MPsltu5DqJf",
        "outputId": "1a56149d-4b24-45ee-cae2-f17c4340e285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "characters:  <tf.RaggedTensor [[b'k', b'e', b'e', b'p'], [b's', b't', b'o', b'r', b'e']]>\n",
            "text:  tf.Tensor([b'keep' b'store'], shape=(2,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "id2char = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True)\n",
        "\n",
        "chars = id2char(ids)\n",
        "print(\"characters: \", chars)\n",
        "\n",
        "# now we want to use a code to join chars into strings\n",
        "# Hint: You should use a method from tf.strings called reduce_join\n",
        "def id2text(ids):\n",
        "  id2char = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True)\n",
        "  chars = id2char(ids)\n",
        "  text = tf.strings.reduce_join(chars, axis=-1)\n",
        "  return text\n",
        "\n",
        "restored_text = id2text(ids)\n",
        "print(\"text: \", restored_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq095wplFrok"
      },
      "source": [
        "## Creating the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v412nM1wFxYT"
      },
      "source": [
        "In this section we would like to divide our text into sequences. Each sequence will be constrained by <font color='red'>sequence_length</font> that we define.\n",
        "\n",
        "The corresponding target sequence of each input sequence has the same length except one character shifted to the right. So you should break the text into chunks of <font color='red'> seq_length+1</font> For instance, given a seq_length of 5 and \"Python\" as text, the input sequence would be \"pytho\" the target would be \"ython\".\n",
        "\n",
        "To do this you should use:\n",
        "```\n",
        "tf.data.Dataset.from_tensor_slices\n",
        "```\n",
        "to convert the text vector into a sequence of character indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrlxMdT_HkUM",
        "outputId": "7c879a0e-1b35-41bc-dbdd-ac64374ea60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "all_ids = char2id(tf.strings.unicode_split(text, input_encoding='UTF-8'))\n",
        "print(all_ids)\n",
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(id2char(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9lToSqBVRd0"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XulJjvGlVS13"
      },
      "source": [
        "Now use the <font color='red'>batch</font> method to convert these characters to sequences with the desired length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG4-tH8IVnKj",
        "outputId": "c1b09487-48a9-4c06-a2b3-4e341da412bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
            "----------------------------------------------------------------------------------------------------------------\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(id2char(seq))\n",
        "\n",
        "print(\"----------------------------------------------------------------------------------------------------------------\")\n",
        "  \n",
        "# Changing tokens back to text\n",
        "for seq in sequences.take(5):\n",
        "  print(id2text(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtSr10QhV47C"
      },
      "source": [
        "To train your model you need aa dataset consisting pairs of (input, label), where input and label are sequences. Given each time step, an input is the current character and the label is the next character. Now write a function that takes a sequence input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1zQmHlrXLgS"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUlKbDkJXWz1"
      },
      "source": [
        "example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70Jit5ZXYIH",
        "outputId": "79b8407d-3898-4a0d-ef16-ee4868684f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['H', 'e', 'l', 'l'], ['e', 'l', 'l', 'o'])\n"
          ]
        }
      ],
      "source": [
        "print(split_input_target(list(\"Hello\")))\n",
        "\n",
        "# Should return:\n",
        "# ''' ([H, e, l, l],\n",
        "#     [e, l, l, o]) '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8s6rg7jYBGH"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCJJu2nWYSzm",
        "outputId": "f1100610-fb01-44a3-b327-93618f57f390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", id2text(input_example).numpy())\n",
        "    print(\"Target:\", id2text(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca7Lmz_ZZKuS"
      },
      "source": [
        "## Creating training batches\n",
        "\n",
        "Now you should shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1X8Jcd_ZsQQ",
        "outputId": "c318b2df-8de0-4cd7-99b5-85a456e74abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size is used for shuffling the dataset\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = (dataset\n",
        "          .shuffle(BUFFER_SIZE)\n",
        "          .batch(BATCH_SIZE, drop_remainder=True)\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnDs0kdaabNl"
      },
      "source": [
        "## Building the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dslxli5ff1E5"
      },
      "source": [
        "Here you should build your model. Please use the following for your model:\n",
        "\n",
        "\n",
        "*   An embedding layer\n",
        "*   An RNN layer (LSTM or GRU)\n",
        "*   Dense Layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRmxCjA5fzxo"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruWjKfGYrpGw",
        "outputId": "96bfa603-5f86-461b-ff49-b01a06fcecd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQw5veIhgZjO"
      },
      "outputs": [],
      "source": [
        "class GenModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-gWS1B1hXRL"
      },
      "outputs": [],
      "source": [
        "model = GenModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(char2id.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRhnsajDhrH6"
      },
      "source": [
        "## Check the model\n",
        "\n",
        "Now let's check our model to see if it behaves as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOn-xz2Khyfr",
        "outputId": "2e06a5a7-49f0-4b15-f033-666a1bd49c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pkL7vAZh3tU",
        "outputId": "95bdf375-503f-4552-8134-42cd91c1d0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"gen_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwzxiHCkh-md",
        "outputId": "306dcba4-d7b4-4637-92a7-c31361f3e672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17  4 17 39 55 44 28 11 31 55 10 50 42 47 12 17 44 15 13 12 37 52 62 14\n",
            " 21 53 61 37 30 50 28 24 30  2 53 47 65 60 33 29 25 31 53 16 48 52 45 27\n",
            " 15 45 57  5  4 39 33 48 30 16 57 32 54 53  1 56  2 41 44 13 47 20 22 10\n",
            " 55 24 29 63  1 51 43 25 44 39 60 47 60 12 61 11 55 64 64 36 61 41 11 48\n",
            " 59 22 55 59]\n"
          ]
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "print(sampled_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHJyMJ3SiEnq",
        "outputId": "b0aa9488-3a12-437b-f2c0-a2ffc96fff16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'l or lose his hire.\\n\\nVIRGILIA:\\nHis bloody brow! O Jupiter, no blood!\\n\\nVOLUMNIA:\\nAway, you fool! it m'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'D$DZpeO:Rp3kch;DeB?;XmwAHnvXQkOKQ nhzuTPLRnCimfNBfr&$ZTiQCrSon\\nq be?hGI3pKPx\\nldLeZuhu;v:pyyWvb:itIpt'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", id2text(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", id2text(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTjxCVnwiL_G"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We have now built our model. The only part left is to train and then test the model.\n",
        "\n",
        "We need a loss function. Please choose the correct loss function from the followings:\n",
        "\n",
        "\n",
        "*   Categorical Cross Entropy\n",
        "*   Sparse Categorical Cross Entropy\n",
        "*   Binary Cross Entropy\n",
        "*   MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0h1o-1bjIS8"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcBTEqIYjM2J",
        "outputId": "079f6760-8e4e-4fba-ac35-cb4aa0862e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.189194\n"
          ]
        }
      ],
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB8PxGcjjZK8"
      },
      "source": [
        "Mean loss with no training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqf_G47XjeEE",
        "outputId": "82664820-8167-4357-d0d7-d757b9aa7306"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.96961"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "tf.exp(mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAVtH6tkjep3"
      },
      "source": [
        "Compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5ypuFc7joKN"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaqY2LBYjhpe"
      },
      "source": [
        "Configure Checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx-ALXaZjmFM"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkfhD3uPjyrL",
        "outputId": "5268d772-5323-4091-848f-4ce13e398700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 24s 125ms/step - loss: 2.7194\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 2.0068\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.7389\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 22s 126ms/step - loss: 1.5715\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.4661\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.3945\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.3385\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.2933\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.2498\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.2100\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 22s 126ms/step - loss: 1.1677\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 22s 126ms/step - loss: 1.1252\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 22s 127ms/step - loss: 1.0802\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 22s 126ms/step - loss: 1.0321\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 22s 126ms/step - loss: 0.9803\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 22s 127ms/step - loss: 0.9278\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 22s 127ms/step - loss: 0.8742\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 22s 127ms/step - loss: 0.8227\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 22s 127ms/step - loss: 0.7732\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 22s 126ms/step - loss: 0.7292\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOXZUCpGkFfa"
      },
      "source": [
        "## Generate Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a57X8y-Kbw-"
      },
      "source": [
        "Here we write a class to generate characters based on the model we trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1u0o-jvmezu"
      },
      "outputs": [],
      "source": [
        "class OneChar(tf.keras.Model):\n",
        "  def __init__(self, model, id2char, char2id, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = id2char\n",
        "    self.ids_from_chars = char2id\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape= [len(char2id.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_char(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    # Convert tokens to ids\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model and get the states and predicted logits\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs (use random.categorical).\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwdouIS9oXz8"
      },
      "outputs": [],
      "source": [
        "one_char_model = OneChar(model, id2char, char2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCxXBl0Moo0I"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qxLixtYoohV",
        "outputId": "60c04dad-a24b-4b83-c46e-351179a815e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Who is't touchinous top? Allow it must needs?\n",
            "\n",
            "Pedant:\n",
            "She holds, it that there is a house! here, sir.\n",
            "\n",
            "PETRUCHIO:\n",
            "Marry, master, play, boy, good Kate, I am nor aimerce;\n",
            "Therefore you scapp me in prison. Now, good sir,\n",
            "You live, and that thou obstury'\n",
            "Wift and a servant of chambet hours haves been\n",
            "many a thorny with this friar by this?\n",
            "\n",
            "ANTONIO:\n",
            "No.\n",
            "\n",
            "HORTENSIO:\n",
            "Sir, your garters, call him for:\n",
            "As Angelo for him; which is from the wings\n",
            "And never than he goes and tears me!\n",
            "Signior Baptista, of your course, I thank thee.\n",
            "\n",
            "GRUMIO:\n",
            "Who, if a sound of the nipel kind? If it,\n",
            "and most delights, I dare not break that combany yields\n",
            "To Bio over and lock-foot.\n",
            "\n",
            "TRANIO:\n",
            "I do: and welcome my brother and the frowers;\n",
            "For he is chief thou shalt bow-like his religat?\n",
            "\n",
            "PETRUCHIO:\n",
            "Mast thou not, go: I say, this, blushes, mine,\n",
            "With unchossess that hath brought my strengh\n",
            "For her chaple. Thou kinsmen groans\n",
            "As Pluboud us by wide commends about to speak.\n",
            "Now well make it here is too shriur.\n",
            "What 'scape  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.806136131286621\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_char_model.generate_one_char(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvmmtzvepHjI"
      },
      "source": [
        "Now play with the model and hyperparameters (epochs, ...) and run this again to see if the results have improved."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Improve model**"
      ],
      "metadata": {
        "id": "suM_i6xGii0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use two GRU layer and increase epoch number to improve model."
      ],
      "metadata": {
        "id": "9Wv_Znz1PeO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgHEyvE2itff"
      },
      "outputs": [],
      "source": [
        "class GenModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru1 = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.gru2 = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states1=None, states2=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    if states1 is None:\n",
        "      states1 = self.gru1.get_initial_state(x)\n",
        "    x, states1 = self.gru1(x, initial_state=states1, training=training)\n",
        "\n",
        "\n",
        "    if states2 is None:\n",
        "      states2 = self.gru2.get_initial_state(x)\n",
        "    x, states2 = self.gru2(x, initial_state=states2, training=training)\n",
        "\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states1, states2\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDttLCLritfh"
      },
      "outputs": [],
      "source": [
        "model = GenModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(char2id.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIfkngspvnjh",
        "outputId": "158a255f-9c9f-4ae5-d615-fd44e2215f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7146fb-004c-4a10-bf2d-30035e447149",
        "id": "kK3Xk1_pitfh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"gen_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  6297600   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,320,450\n",
            "Trainable params: 10,320,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xDv_q4Xitfi"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8c66a7-5314-426d-b149-a52cf5e63d58",
        "id": "C-qWuvVpitfi"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "172/172 [==============================] - 56s 304ms/step - loss: 2.5894\n",
            "Epoch 2/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 1.8226\n",
            "Epoch 3/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 1.5659\n",
            "Epoch 4/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 1.4357\n",
            "Epoch 5/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 1.3579\n",
            "Epoch 6/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 1.3001\n",
            "Epoch 7/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 1.2510\n",
            "Epoch 8/35\n",
            "172/172 [==============================] - 52s 303ms/step - loss: 1.2047\n",
            "Epoch 9/35\n",
            "172/172 [==============================] - 52s 303ms/step - loss: 1.1568\n",
            "Epoch 10/35\n",
            "172/172 [==============================] - 52s 303ms/step - loss: 1.1062\n",
            "Epoch 11/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 1.0480\n",
            "Epoch 12/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.9843\n",
            "Epoch 13/35\n",
            "172/172 [==============================] - 52s 303ms/step - loss: 0.9119\n",
            "Epoch 14/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.8328\n",
            "Epoch 15/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.7531\n",
            "Epoch 16/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.6707\n",
            "Epoch 17/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.5942\n",
            "Epoch 18/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.5229\n",
            "Epoch 19/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.4627\n",
            "Epoch 20/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.4091\n",
            "Epoch 21/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.3684\n",
            "Epoch 22/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.3296\n",
            "Epoch 23/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.3041\n",
            "Epoch 24/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.2828\n",
            "Epoch 25/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.2687\n",
            "Epoch 26/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.2556\n",
            "Epoch 27/35\n",
            "172/172 [==============================] - 52s 303ms/step - loss: 0.2478\n",
            "Epoch 28/35\n",
            "172/172 [==============================] - 52s 303ms/step - loss: 0.2419\n",
            "Epoch 29/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.2369\n",
            "Epoch 30/35\n",
            "172/172 [==============================] - 52s 303ms/step - loss: 0.2331\n",
            "Epoch 31/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.2339\n",
            "Epoch 32/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.2334\n",
            "Epoch 33/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.2320\n",
            "Epoch 34/35\n",
            "172/172 [==============================] - 52s 302ms/step - loss: 0.2257\n",
            "Epoch 35/35\n",
            "172/172 [==============================] - 52s 303ms/step - loss: 0.2227\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 35\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneChar(tf.keras.Model):\n",
        "  def __init__(self, model, id2char, char2id, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = id2char\n",
        "    self.ids_from_chars = char2id\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape= [len(char2id.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_char(self, inputs, states1=None, states2=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    # Convert tokens to ids\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model and get the states and predicted logits\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states1, states2 = self.model(inputs=input_ids, states1=states1, states2=states2, return_state=True)\n",
        "\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs (use random.categorical).\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states1, states2"
      ],
      "metadata": {
        "id": "WyZbPWANZgwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_char_model = OneChar(model, id2char, char2id)"
      ],
      "metadata": {
        "id": "4GzUGisvscpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsRiYKTLpU8H",
        "outputId": "15eb36f2-623e-4af7-da65-b6a80fa1d604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "There is thy mother roar'd.\n",
            "\n",
            "PROSPERO:\n",
            "O, a cherubim\n",
            "That which I should come to me than Tybalt's death,\n",
            "There to the fruitor to me for her inclination\n",
            "And bid good morrow; thy riches it at the heel,\n",
            "And three renowned me and consent.\n",
            "\n",
            "LUCENTIO:\n",
            "I fly, Biondello: but they know me.\n",
            "\n",
            "MENENIUS:\n",
            "Do you hear, sir?\n",
            "\n",
            "PETRUCHIO:\n",
            "Very well; I see thee ill-spirit,\n",
            "Stand and speak of some merry passion\n",
            "And so of me, where we cracking the world?\n",
            "This night's the time I will unto Venice,\n",
            "To buy apparel 'gainst the Frederick was wrick'd upon\n",
            "The duke is entering: then, masters, to whip her kind\n",
            "Our trunken sleep; commanded likence's vagad,\n",
            "To whit here an embraces have brought him.\n",
            "\n",
            "HERMIONE:\n",
            "No matter, gentlemen! ladies; but I know her with\n",
            "Respected; every old friends,\n",
            "More fiery lord will prove a second Griar.\n",
            "I not been mean an argosy.\n",
            "\n",
            "TRANIO:\n",
            "If it be so, sir, to great her grave.\n",
            "\n",
            "PETRUCHIO:\n",
            "You lie, in fair of lustful applause and\n",
            "Master's one and Saint George, I am a gentleman.\n",
            "\n",
            "KATHARINA:\n",
            " \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 8.237735986709595\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states1 = None\n",
        "states2 = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states1, states2 = one_char_model.generate_one_char(next_char, states1=states1, states2=states2)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}