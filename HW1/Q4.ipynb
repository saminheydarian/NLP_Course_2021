{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_validation(string):\n",
    "    pattern  = '^[A-z0-9_\\.]+@[A-z0-9]+\\.([A-z]){3}$'\n",
    "\n",
    "#     output = re.fullmatch(pattern, string)\n",
    "    output = re.match(pattern, string)\n",
    "\n",
    "    if output:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "string_1 = 'samin.heydarian97@gmail.com'\n",
    "string_2 = 'samin.heydarian97@gmail.comx'\n",
    "string_3 = 'samin.heydarian97gm_il.com'\n",
    "string_4 = 'samin.heydarian97@gmail.co9'\n",
    "\n",
    "output = email_validation(string_1)\n",
    "print(output)\n",
    "\n",
    "output = email_validation(string_2)\n",
    "print(output)\n",
    "\n",
    "output = email_validation(string_3)\n",
    "print(output)\n",
    "\n",
    "output = email_validation(string_4)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phoneNumber_validation(string):\n",
    "    pattern  = '^(09|\\+989|00989)([0-9]){9}$'\n",
    "\n",
    "    output = re.match(pattern, string)\n",
    "\n",
    "    if output:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "string_1 = '09223244139'\n",
    "string_2 = '++989223244139'\n",
    "string_3 = 's9223244139'\n",
    "string_4 = '00089223244139'\n",
    "\n",
    "output = phoneNumber_validation(string_1)\n",
    "print(output)\n",
    "\n",
    "output = phoneNumber_validation(string_2)\n",
    "print(output)\n",
    "\n",
    "output = phoneNumber_validation(string_3)\n",
    "print(output)\n",
    "\n",
    "output = phoneNumber_validation(string_4)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ['\\u200cها',\n",
    "          'ها',\n",
    "          'ان',\n",
    "          'ات',\n",
    "          'م',\n",
    "          'ه\\u200c',\n",
    "          'ه',\n",
    "          'ه\\u200cاند',\n",
    "          'ه\\u200cبودند',\n",
    "          'ید',\n",
    "          'ند']\n",
    "\n",
    "stemmer_suffixes = ['\\u200cها',\n",
    "          'ها',\n",
    "          'ان',\n",
    "          'ات',\n",
    "          'م',\n",
    "          '\\u200cاند',\n",
    "          'بودند',\n",
    "          'ید',\n",
    "          'ند']\n",
    "\n",
    "prefixes = ['می\\u200c']\n",
    "\n",
    "exceptions = ['بوستان']\n",
    "\n",
    "root = {'خورد':'خور',\n",
    "       'خور':'خور',\n",
    "       'رفت':'رو',\n",
    "       'رو':'رو',\n",
    "       'گفت':'گو',\n",
    "       'گو':'گو',\n",
    "       'گوی':'گو',\n",
    "       'نوشت':'نویس'}\n",
    "\n",
    "def lemmatization(string):\n",
    "    if string not in exceptions:\n",
    "        for suffix in suffixes:\n",
    "            if string.endswith(suffix):\n",
    "                lemmatized_string = string[:-len(suffix)]\n",
    "                string = lemmatized_string\n",
    "                break\n",
    "        \n",
    "        for prefix in prefixes:\n",
    "            if string.startswith(prefix):\n",
    "                lemmatized_string = string[len(prefix):]\n",
    "                string = lemmatized_string\n",
    "                break\n",
    "        \n",
    "        if string in root:\n",
    "            return root[string]\n",
    "        else:\n",
    "            return string\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary = ['آنها',\n",
    "                  'کتاب\\u200cها',\n",
    "                  'بوستان',\n",
    "                  'درختان',\n",
    "                  'فردا',\n",
    "                  'اتصالات',\n",
    "                  'می\\u200cروم',\n",
    "                  'رفته\\u200cاند',\n",
    "                  'گفته\\u200cبودند',\n",
    "                  'می\\u200cگویید',\n",
    "                  'می\\u200cخورم',\n",
    "                  'خوردند',\n",
    "                  'نوشتید']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "آن\n",
      "کتاب\n",
      "بوستان\n",
      "درخت\n",
      "فردا\n",
      "اتصال\n",
      "رو\n",
      "رو\n",
      "گو\n",
      "گو\n",
      "خور\n",
      "خور\n",
      "نویس\n"
     ]
    }
   ],
   "source": [
    "for word in word_dictionary:\n",
    "    print(lemmatization(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(string):\n",
    "    if string not in exceptions:\n",
    "        for suffix in stemmer_suffixes:\n",
    "            if string.endswith(suffix):\n",
    "                stemmed_string = string[:-len(suffix)]\n",
    "                string = stemmed_string\n",
    "                break\n",
    "        \n",
    "        for prefix in prefixes:\n",
    "            if string.startswith(prefix):\n",
    "                stemmed_string = string[len(prefix):]\n",
    "                string = stemmed_string\n",
    "                break\n",
    "        return string\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "آن\n",
      "کتاب\n",
      "بوستان\n",
      "درخت\n",
      "فردا\n",
      "اتصال\n",
      "رو\n",
      "رفته\n",
      "گفته‌\n",
      "گوی\n",
      "خور\n",
      "خورد\n",
      "نوشت\n"
     ]
    }
   ],
   "source": [
    "for word in word_dictionary:\n",
    "    print(stemmer(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_stem_lemmatize(input_sentence):\n",
    "    tokens = []\n",
    "    simple_tokens = input_sentence.replace(\"،\", \"\").split()\n",
    "    for token in simple_tokens:\n",
    "        stemmed = stemmer(token)\n",
    "        lemmatized = lemmatization(stemmed)\n",
    "        tokens.append(lemmatized)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'کتاب', 'گو', 'نویس', 'فردا', 'اتصال', '</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"<s> کتاب\\u200cها، گفته\\u200cبودند، نوشتید، فردا، اتصالات </s>\"\n",
    "tokenize_stem_lemmatize(input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [\n",
    "    \"<s> کتاب\\u200cها، گفته\\u200cبودند، نوشتید، فردا، اتصالات </s>\",\n",
    "    \"<s> کتاب\\u200cها، آن\\u200cها، درختان، می\\u200cروم، می\\u200cگویید </s>\",\n",
    "    \"<s> می\\u200cخورم، می\\u200cگویید، نوشتید، اتصالات </s>\",\n",
    "    \"<s> آن\\u200cها، درختان، رفته\\u200cاند، کتاب\\u200cها، اتصالات </s>\",\n",
    "    \"<s> کتاب\\u200cها، رفته\\u200cاند، می\\u200cخورم، نوشتید </s>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_grams(input_sentence):\n",
    "    tokens = tokenize_stem_lemmatize(input_sentence)\n",
    "    one_grams_list = []\n",
    "    for i in tokens:\n",
    "        one_grams_list.append(i)\n",
    "        \n",
    "    return one_grams_list\n",
    "\n",
    "def all_one_grams(input_set):\n",
    "    one_grams_dictionary = {}\n",
    "    for input_sentence in input_set:\n",
    "        one_grams_list = one_grams(input_sentence)\n",
    "        for one_gram in one_grams_list:\n",
    "            if one_gram in one_grams_dictionary:\n",
    "                one_grams_dictionary[one_gram] += 1\n",
    "            else:\n",
    "                one_grams_dictionary[one_gram] = 1\n",
    "    return one_grams_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'کتاب', 'گو', 'نویس', 'فردا', 'اتصال', '</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_grams(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 5,\n",
       " 'کتاب': 4,\n",
       " 'گو': 3,\n",
       " 'نویس': 3,\n",
       " 'فردا': 1,\n",
       " 'اتصال': 3,\n",
       " '</s>': 5,\n",
       " 'آن': 2,\n",
       " 'درخت': 2,\n",
       " 'رو': 3,\n",
       " 'خور': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_grams_dictionary = all_one_grams(training_set)\n",
    "one_grams_dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_grams(input_sentence):\n",
    "    tokens = tokenize_stem_lemmatize(input_sentence)\n",
    "    bi_grams_list = []\n",
    "    for i in range(len(tokens)-1):\n",
    "        bi_grams_list.append([tokens[i],tokens[i+1]])\n",
    "        \n",
    "    return bi_grams_list\n",
    "\n",
    "def all_bi_grams(input_set):\n",
    "    bi_grams_dictionary = {}\n",
    "    for input_sentence in input_set:\n",
    "        bi_grams_list = bi_grams(input_sentence)\n",
    "        for bi_gram in bi_grams_list:\n",
    "            bi_gram = bi_gram[0] + \"*\" + bi_gram[1]\n",
    "            if bi_gram in bi_grams_dictionary:\n",
    "                bi_grams_dictionary[bi_gram] += 1\n",
    "            else:\n",
    "                bi_grams_dictionary[bi_gram] = 1\n",
    "    return bi_grams_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'کتاب'],\n",
       " ['کتاب', 'گو'],\n",
       " ['گو', 'نویس'],\n",
       " ['نویس', 'فردا'],\n",
       " ['فردا', 'اتصال'],\n",
       " ['اتصال', '</s>']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_grams(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>*کتاب': 3,\n",
       " 'کتاب*گو': 1,\n",
       " 'گو*نویس': 2,\n",
       " 'نویس*فردا': 1,\n",
       " 'فردا*اتصال': 1,\n",
       " 'اتصال*</s>': 3,\n",
       " 'کتاب*آن': 1,\n",
       " 'آن*درخت': 2,\n",
       " 'درخت*رو': 2,\n",
       " 'رو*گو': 1,\n",
       " 'گو*</s>': 1,\n",
       " '<s>*خور': 1,\n",
       " 'خور*گو': 1,\n",
       " 'نویس*اتصال': 1,\n",
       " '<s>*آن': 1,\n",
       " 'رو*کتاب': 1,\n",
       " 'کتاب*اتصال': 1,\n",
       " 'کتاب*رو': 1,\n",
       " 'رو*خور': 1,\n",
       " 'خور*نویس': 1,\n",
       " 'نویس*</s>': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_grams_dictionary = all_bi_grams(training_set)\n",
    "bi_grams_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_p_calculator(input_sentence):\n",
    "    p_list = []\n",
    "    bi_grams_list = bi_grams(input_sentence)\n",
    "    vocab_size = len(one_grams_dictionary) - 1\n",
    "\n",
    "    for bi_gram_first_part, bi_gram_second_part in bi_grams_list:\n",
    "        bigram = bi_gram_first_part+\"*\"+bi_gram_second_part\n",
    "        if bigram in bi_grams_dictionary:\n",
    "            no_of_together_occurrence = bi_grams_dictionary[bigram]\n",
    "        else:\n",
    "            no_of_together_occurrence = 0\n",
    "        if bi_gram_first_part in one_grams_dictionary:\n",
    "            no_of_first_part_occurrence = one_grams_dictionary[bi_gram_first_part]\n",
    "        else:\n",
    "            no_of_first_part_occurrence = 0\n",
    "            \n",
    "        p = np.divide((no_of_together_occurrence+1),(no_of_first_part_occurrence+vocab_size))\n",
    "        p_list.append(p)\n",
    "    \n",
    "    return np.prod(p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    \"<s> کتاب\\u200cها، می\\u200cگویید، می\\u200cخورم، بوستان، اتصالات </s>\",\n",
    "    \"<s> بوستان، رفته\\u200cاند، کتاب\\u200cها، می\\u200cگویید، می\\u200cگویید </s>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.513853667699822e-06"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_p_calculator(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7339662310076517e-06"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_p_calculator(test_set[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
