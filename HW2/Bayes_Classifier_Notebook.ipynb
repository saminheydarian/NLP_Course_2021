{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayes_Classifier_Notebook_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3v9p59dC-5y"
      },
      "source": [
        "# Intro\n",
        "In the Name of Allah\n",
        "\n",
        "Sentiment analysis is a technique through which you can analyze a piece of text to determine the sentiment behind it. In this notebook, we're going to train a Naïve Bayes Classifier for the task of sentiment analysis on Imdb movie reviews dataset.\n",
        "\n",
        "**Please pay attention to these notes:**\n",
        "\n",
        "<br/>\n",
        "\n",
        "- **Assignment Due:** 1400/09/19 23:59\n",
        "- Write your code in the cells denoted by:\n",
        "```\n",
        "######## Your Code Here ########\n",
        "```\n",
        "- You can add more cells if necessary\n",
        "- Finding any sort of copying will zero down your grade.\n",
        "- When your solution is ready to submit, don't forget to set the name of this notebook like  \"Name_StudentID.ipynb\".\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You can also ask your questions on the telegram group.\n",
        "- You must run this notebook on Google Colab platform.\n",
        "\n",
        "<br/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iryuq7HoLy3j"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-R7b4iAuKnP"
      },
      "source": [
        "# importing the libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import collections\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split as tts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj9uxuvswOpA"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqW0otKAQWB9"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ujxrR8QkeU"
      },
      "source": [
        "######## Your Code Here ########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "370oVEkqe8mp"
      },
      "source": [
        "# Preprocess\n",
        "The first step of NLP is text preprocessing. Data cleaning is a very crucial step in any machine learning model, but more so for NLP. Without the cleaning process, the dataset is often a cluster of words that the computer doesn’t understand. Raw data over a properly or improperly formed sentence is not always desirable as it contains lot of unwanted components like null/html/links/url/emoji/stopwords etc. So in this step, this unwanted components are removed for better performance and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWID_NYBk1oN"
      },
      "source": [
        "######## Your Code Here ########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeB0RNlJ89_g"
      },
      "source": [
        "<font size=\"5\">Split the dataset</font>\n",
        "\n",
        "Data splitting, or commonly known as train-test split, is the partitioning of data into subsets for model training and evaluation separately. Since the test set is not specified beforehand, we have to split the dataset into train and test set in an ideal proportion. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZT57Dblyf8Y"
      },
      "source": [
        "######## Your Code Here ########\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnesufkigiPw"
      },
      "source": [
        "# Training\n",
        "Use Naive Beyes algorithm to train a Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf2dHGlKurSf"
      },
      "source": [
        "######## Your Code Here ########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvSqU886lMDN"
      },
      "source": [
        "# Test\n",
        "Now you need to run inference on your test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pr2rqDwlPpU"
      },
      "source": [
        "######## Your Code Here ########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBS9HFA8iWlX"
      },
      "source": [
        "# Evaluation\n",
        "After training is finished, we need some metrics to evaluate the trained model on the test set. Here, you need to write code for utilizing the metrics bellow without the sklearn libraries!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIeLqza_llil"
      },
      "source": [
        "Precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjzeJiZWloV8"
      },
      "source": [
        "######## Your Code Here ########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go2HcCbTln_M"
      },
      "source": [
        "Recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phSzQZ6JlsV1"
      },
      "source": [
        "######## Your Code Here ########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUM6OQNAlqks"
      },
      "source": [
        "F-measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnzV0G1tlsA8"
      },
      "source": [
        "######## Your Code Here ########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmo7-ZsEytmo"
      },
      "source": [
        "Confustion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVFCeotJy1DZ"
      },
      "source": [
        "######## Your Code Here ########"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}